# 跟踪任务设计与奖励函数文档

## 1. 任务概述
**目标**：将“跟踪（Tracking）”任务调整为**农业采摘机器人**场景。目标是让移动机械臂高效且安全地到达一个**静态目标**（代表水果或作物）。

## 2. 环境逻辑

### 2.1 物体生成（环形分布）
目标物体静态生成在机器人周围的一个环形区域内，以确保它既可达又需要一定的移动。
- **半径范围**：$r \in [0.5m, 1.0m]$
- **角度**：随机 $\theta \in [0, 2\pi]$
- **高度**：固定或微调（基于物体类型）。

**代码参考 (`gym_dcmm/envs/DcmmVecEnv.py`):**
```python
# 在 [0.5, 1.0] 范围内采样半径
r = np.random.uniform(0.5, 1.0)
theta = np.random.uniform(0, 2 * np.pi)
self.object_init_pos[0] = r * np.cos(theta) + self.arm_base_pos[0]
self.object_init_pos[1] = r * np.sin(theta) + self.arm_base_pos[1]
```

### 2.2 静态物体物理
为了模拟静态作物，物体的物理属性受到限制：
- **速度**：在物理引擎中强制为零（或通过高阻尼/质量实现）。
- **观测**：在观测空间中将物体的速度显式设为零，以防止智能体产生运动幻觉。

**代码参考 (`gym_dcmm/envs/DcmmVecEnv.py`):**
```python
# 在观测中强制物体速度为零
obs["object"]["v_lin_3d"] = np.zeros(3)
```

## 3. 奖励函数设计

奖励函数旨在解决“稀疏奖励”问题，并通过提供强劲、恒定的梯度来防止“懒惰智能体（Lazy Agent）”问题。

### 3.1 到达奖励（线性正向整形）
**公式**：$R_{reaching} = \max(0, 5.0 - d)$
- **$d$**：末端执行器（End-Effector）与目标之间的欧几里得距离。
- **梯度**：恒定斜率为 **1.0**。每靠近 $10cm$ 获得 $+0.1$ 奖励。
- **范围**：$[0, 5.0]$。触摸时（$d=0$）最大奖励为 $5.0$。
- **目的**：无论距离远近（在5米以内），都提供强劲、一致的驱动力，激励智能体移动。

**代码参考 (`gym_dcmm/envs/DcmmVecEnv.py`):**
```python
reward_reaching = max(0.0, 5.0 - info["ee_distance"])
```

### 3.2 触摸奖励（成功红利）
**数值**：$+10.0$
- **条件**：当机器人的末端执行器接触到目标物体时触发。
- **目的**：提供一个显著超过最大到达奖励（$5.0$）的红利，确保智能体有强烈的动机去进行接触，而不仅仅是在附近徘徊。

**代码参考 (`configs/env/DcmmCfg.py`):**
```python
"r_touch": {
    'Tracking': 10,  # 从 5 增加到 10
    'Catching': 0.1
}
```

### 3.3 碰撞惩罚
**数值**：$-10.0$
- **条件**：如果机器人与环境（地面、自身）或无效物体发生碰撞时触发。
- **目的**：抑制不安全的行为。

### 3.4 总奖励
$$R_{total} = R_{reaching} + R_{touch} + R_{collision}$$

## 4. 关键配置参数

| 参数 | 数值 | 文件 | 描述 |
| :--- | :--- | :--- | :--- |
| `env_time` | **4.0 s** | `train_DCMM.py` | 从 2.5s 增加，给智能体充足的时间到达目标。 |
| `r_ee_pos` | **1.0** | `DcmmCfg.py` | 距离奖励的权重（现在隐含在线性公式中）。 |
| `r_touch` | **10.0** | `DcmmCfg.py` | 成功触摸的额外奖励。 |
| `steps_per_policy` | 20 | `train_DCMM.py` | 每个RL步对应的物理步数。 |

## 5. 设计意图总结
1.  **动机（Motivation）**：**线性正向奖励**保证了“靠近”总是能带来正收益，解决了“懒惰智能体”问题（即移动成本可能超过微小的距离收益）。
2.  **耐心（Patience）**：将 `env_time` 增加到 **4.0s**，让初期“笨拙”的策略有足够的时间跌跌撞撞地找到目标，促进早期学习。
3.  **清晰度（Clarity）**：在观测中将物体速度归零，消除了噪声，帮助智能体专注于位置控制。
