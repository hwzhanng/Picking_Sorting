seed: ${..seed}
algo: PPO

network:
  mlp:
    units: [256, 128]
    # units_track: [512, 256, 128]
    # units_catch: [128, 64]
  separate_value_mlp: True

ppo:
  name: ${resolve_default:Dcmm,${...experiment}}
  normalize_input: True
  normalize_value: True
  value_bootstrap: True
  num_actors: ${resolve_default:8,${...num_envs}}
  reward_scale_value: 0.1
  clip_value_loss: False
  normalize_advantage: True
  gamma: 0.99
  tau: 0.95
  
  # [Modified] Lower learning rate for stability
  learning_rate: 3e-4
  lr_schedule: kl
  kl_threshold: 0.016
  
  save_best_after: 500
  save_frequency: 2000
  
  # [Modified] Stricter gradient clipping
  grad_norm: 0.5 
  
  # [Critical] Enable entropy regularization to prevent early convergence to "playing dead"
  entropy_coef: 0.01 
  
  freeze_base: False
  freeze_arm: False
  truncate_grads: True
  e_clip: 0.2
  action_track_denorm: [1.5, 0.025, 0.01]
  action_catch_denorm: [1.5, 0.025, 0.15]
  
  # [Modified] Increase horizon length for long-term dependency (obstacle avoidance)
  horizon_length: 512
  
  # Ensure (num_actors * horizon_length) / minibatch_size is integer
  minibatch_size: 512
  mini_epochs: 6
  
  critic_coef: 4
  clip_value: True
  bounds_loss_coef: 0.0001
  max_agent_steps: 25000000
  test_num_episodes: 100
  max_test_steps: 10000
  img_dim: [224, 224]
  num_frames: 2
