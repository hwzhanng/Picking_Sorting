# Catch It 项目架构详细说明文档

## 📋 项目概述

**Catch It** 是一个基于强化学习的移动农业采摘项目,采用两阶段训练方法训练移动操作机器人(配备移动底盘、6自由度机械臂和灵巧手)自主导航至目标并进行采摘。项目使用 MuJoCo 物理引擎进行仿真,采用 PPO (Proximal Policy Optimization) 算法,并集成了**动态课程学习 (Curriculum Learning)** 机制。

### 🆕 最新更新亮点

- **文件系统重构 (2025-12)**: 完全分离 Stage 1 和 Stage 2文件结构,实现独立开发
- **农业场景适配**: 从通用抓取任务调整为农业果实采摘场景(静态目标)
- **动态课程学习**: 实现从"探索"到"约束"的渐进式训练难度调节
- **精细化奖励函数**: 重构奖励系统,区分树干/树叶碰撞,优化手掌朝向奖励
- **关节空间控制**: 从IK控制切换到关节空间直接控制,提升稳定性
- **视觉鲁棒性增强**: 增加深度图噪声和像素缺失模拟
- **🆕 Stage 2 核心优化 (2025-12)**:
  - **内存优化**: 深度缓冲区使用 `uint8` 存储,节省 75% GPU 显存 (220MB→55MB)
  - **扰动测试**: 外力验证抓取稳定性 (2-5N 力,滑移 < 1cm 奖励 +10.0)
  - **速度惩罚加强**: 系数调至 `-6.0`,防止冲击式接触 (0.5m/s 下惩罚 -10.3)

---

## 📁 根目录文件说明

### 1. **README.md**
- **作用**: 项目说明文档
- **功能**: 
  - 介绍项目核心特性和两阶段训练框架
  - 提供安装指南和使用说明
  - 说明追踪(Tracking)和抓取(Catching)两个训练阶段的目标和机制

### 2. **setup.py**
- **作用**: Python 包安装脚本
- **功能**:
  - 定义项目名称为 `gym_dcmm` (Gymnasium environment for Dexterous Catch with Mobile Manipulation)
  - 指定依赖包:`gymnasium==0.29.1` 和 `mujoco>=3.0.0`
  - 配置包的安装参数和元信息
  - 支持通过 `pip install -e .` 进行开发模式安装

### 3. **requirements.txt**
- **作用**: Python 依赖包列表
- **功能**: 列出项目所需的所有第三方库及版本,包括:
  - **数值计算**: numpy, scipy
  - **图像处理**: opencv-python, imageio
  - **机器人学**: spatialmath-python, numpy-quaternion
  - **优化求解**: qpsolvers, quadprog
  - **配置管理**: PyYAML, omegaconf, hydra-core
  - **实验记录**: wandb, tensorboardX
  - **其他工具**: termcolor, decorators, seaborn

### 4. **train_stage1.py** 和 **train_stage2.py**
- **作用**: Stage 1 和 Stage 2 的独立训练脚本
- **功能**:
  - **train_stage1.py**: 训练追踪任务(Tracking)
    - 使用 `configs/config_stage1.yaml` 配置
    - 实例化 `PPO_Stage1` 智能体
    - 注册并使用 `gym_dcmm/DcmmVecWorld-v0` 环境
  - **train_stage2.py**: 训练抓取任务(Catching)
    - 使用 `configs/config_stage2.yaml` 配置
    - 实例化 `PPO_Stage2` 智能体
    - 注册并使用 `gym_dcmm/DcmmVecWorldCatch-v0` 环境
  - 使用 Hydra 框架管理配置
  - 支持训练和测试模式切换
  - 集成 WandB 进行实验跟踪和可视化
  - 自动保存模型检查点到 `outputs/` 目录

### 5. **test_env.py**
- **作用**: 环境测试脚本
- **功能**:
  - 验证 DcmmVecEnv 环境是否正常初始化
  - 测试环境的 reset 和 step 功能
  - 检查观测空间(深度图像、状态向量)的形状和数值范围
  - 用于调试环境配置和接口

### 6. **MUJOCO_LOG.TXT**
- **作用**: MuJoCo 仿真日志文件
- **功能**: 记录 MuJoCo 物理引擎的运行日志、警告和错误信息

### 7. **项目架构分析.md**
- **作用**: 项目架构分析文档(已存在)
- **功能**: 提供项目结构的简要分析

---

## 📂 主要目录结构详解

## 1️⃣ **configs/** - 配置文件目录

### 1.1 **configs/config_stage1.yaml** 和 **configs/config_stage2.yaml**
- **作用**: Stage 1 和 Stage 2 的主配置文件
- **功能**:
  - **config_stage1.yaml** (Stage 1 追踪任务):
    - 定义实验基本参数:随机种子、设备配置(CPU/GPU)
    - 设置任务类型: `task: Tracking`
    - 配置环境参数:并行环境数量、是否使用视觉输入、是否显示可视化
    - 指定检查点路径:预训练模型的加载路径
    - WandB 配置:实验跟踪、项目名称(`RL_Dcmm_Track_Random`)
    - 默认训练配置: `train/stage1/PPO_Stage1`
  - **config_stage2.yaml** (Stage 2 抓取任务):
    - 设置任务类型: `task: Catching_TwoStage`
    - WandB 项目名称: `RL_Dcmm_Catch_Random`
    - 默认训练配置: `train/stage2/PPO_Stage2`
  - Hydra 框架配置:输出目录设置

### 1.2 **configs/env/** - 环境配置子目录

#### **configs/env/DcmmCfg.py**
- **作用**: 环境核心配置文件(Stage 1 和 Stage 2 共享)
- **功能**:
  - **路径定义**: 
    - ASSET_PATH: 资源文件根路径
    - XML 模型路径:机器人和场景的 XML 描述文件
  - **初始状态配置**:
    - arm_joints: 机械臂初始关节角度(6个关节)
    - hand_joints: 灵巧手初始关节角度(16个关节)
  - **奖励权重**:
    - r_base_pos: 底盘位置奖励
    - r_ee_pos: 末端执行器位置奖励
    - r_precision: 精确度奖励
    - r_orient: 方向奖励
    - r_touch: 接触奖励(追踪和抓取任务不同)
    - r_constraint: 约束奖励
    - r_stability: 稳定性奖励
    - r_ctrl: 控制惩罚(底盘、机械臂、灵巧手分别设置)
    - r_collision: 碰撞惩罚
    - r_finger_approach: 手指接近奖励
    - r_force_closure: 力闭合奖励
  - **🆕 课程学习配置 (curriculum)**:
    - stage1_steps: 第一阶段步数阈值(2M步)
    - stage2_steps: 第二阶段步数阈值(6M步)
    - collision_stem_start/end: 树干碰撞惩罚范围(-1.0 → -20.0)
    - orient_power_start/end: 朝向精度要求范围(1.0 → 4.0次方)
  - **任务参数**:
    - distance_thresh: 从追踪阶段切换到抓取阶段的距离阈值
  - **PID 控制参数**: 底盘驱动、机械臂、灵巧手的 PID 增益和限制

### 1.3 **configs/train/** - 训练配置子目录

#### **configs/train/stage1/PPO_Stage1.yaml**
- **作用**: Stage 1 (Tracking) PPO 算法训练参数配置
- **功能**:
  - **网络架构**:
    - MLP 隐藏层单元数:[256, 128]
    - 是否使用独立的价值网络
    - 使用视觉特征提取(CNN)
  - **PPO 超参数**:
    - 学习率:5e-4
    - 折扣因子 gamma:0.99
    - GAE lambda (tau):0.95
    - Clip 范围:0.2
    - 熵系数:0.0
    - 价值函数系数:4
  - **训练设置**:
    - 水平长度 (horizon_length):64
    - Mini-batch 大小:512
    - Mini-epochs:6
    - 最大训练步数:25,000,000
  - **动作归一化参数**:
    - action_track_denorm: 追踪任务动作反归一化 [底盘1.5, 末端执行器0.025, 手部0.01]
  - **图像配置**:
    - 图像尺寸:[224, 224]

#### **configs/train/stage2/PPO_Stage2.yaml**
- **作用**: Stage 2 (Catching) PPO 算法训练参数配置 
- **功能**:
  - 类似 Stage 1 配置,但针对抓取任务优化
  - **动作归一化参数**:
    - action_catch_denorm: 抓取任务动作反归一化 [1.5, 0.025, 0.15]

---

## 2️⃣ **gym_dcmm/** - 核心环境包

这是项目的核心模块,包含环境定义、智能体控制、算法实现和工具函数。

### 2.1 **gym_dcmm/__init__.py**
- **作用**: 包初始化文件
- **功能**:
  - 注册 Gymnasium 环境 `gym_dcmm/DcmmVecWorld-v0`
  - 指定入口点为 `gym_dcmm.envs:DcmmVecEnv`

### 2.2 **gym_dcmm/envs/** - 环境实现目录

#### **🆕 环境目录结构**
```
gym_dcmm/envs/
├── stage1/                    # Stage 1 (Tracking) 环境
│   ├── DcmmVecEnvStage1.py   # Stage 1 主环境类
│   └── RewardManagerStage1.py # Stage 1 奖励管理器
├── stage2/                    # Stage 2 (Catching) 环境
│   ├── DcmmVecEnvStage2.py   # Stage 2 主环境类
│   └── RewardManagerStage2.py # Stage 2 奖励管理器
└── (共享组件)
    ├── observation_manager.py
    ├── control_manager.py
    ├── randomization_manager.py
    ├── render_manager.py
    └── constants.py
```

#### **DcmmVecEnvStage1.py** 和 **DcmmVecEnvStage2.py**
- **作用**: Stage 1 和 Stage 2 的主环境类(向量化环境)
- **功能**:
  - 实现 Gymnasium 接口:reset(), step(), render()
  - **Stage 1** (Tracking):
    - 支持 Tracking 任务模式
    - 手掌固定为张开姿态
  - **Stage 2** (Catching):
    - 支持 Catching 任务模式
    - 灵巧手可训练
  - 提供多种渲染模式:rgb_array、depth_array、depth_rgb_array
  - 集成各个管理器模块:观测、奖励、随机化、控制、渲染
  - 定义观测空间和动作空间
  - 处理动作延迟缓冲(模拟真实机器人延迟)
  - **🆕 课程学习调度器**:
    - `update_curriculum_difficulty()`: 根据 global_step 动态调整难度参数
    - `set_global_step()`: 接收训练脚本传入的全局步数
    - 每10k步打印当前课程参数(调试用)

#### **observation_manager.py**
- **作用**: 观测管理器(Stage 1 和 Stage 2 共享)
- **功能**:
  - 收集机器人状态:底盘速度、机械臂关节状态、灵巧手关节状态
  - 处理目标物体信息:位置、速度(农业场景下速度置零)
  - 计算相对位置和方向
  - **🆕 深度图鲁棒性增强**:
    - 添加高斯噪声模拟传感器不确定性
    - 模拟深度孔洞(5-10%像素缺失)
  - 坐标系转换(世界坐标系 ↔ 基座坐标系)

#### **RewardManagerStage1.py** 和 **RewardManagerStage2.py**
- **作用**: Stage 1 和 Stage 2 的独立奖励计算管理器
- **🆕 功能**(大幅重构):
  1. **到达奖励 (Reaching Reward)**:
     - 使用 `tanh` 归一化,防止奖励爆炸
     - `reward = 1.0 * (1.0 - tanh(2.0 * distance))`
  2. **底盘定位奖励 (Base Approach Reward)**:
     - 引导底盘停在最佳距离(0.8m)
     - `reward = exp(-5.0 * (distance - 0.8)^2)`
  3. **🆕 朝向奖励 (Orientation Reward)**:
     - 要求手掌朝向目标(palm forward)
     - **动态严格度**:使用 `current_orient_power` (1.0→4.0次方)
     - `reward = max(0, alignment)^orient_power * 2.0`
  4. **接触奖励 (Touch Reward)**:
     - 基础奖励:10.0
     - 速度惩罚:-4.0 * impact_speed (鼓励温柔接触)
  5. **正则化惩罚 (Regularization)**:
     - 控制平滑性:-0.01 * ||ctrl||
  6. **碰撞惩罚 (Collision)**:
     - 底盘碰撞地面:-10.0 (终止)
  7. **🆕 植物碰撞惩罚 (Plant Collision)**:
     - **树干碰撞 (Stem)**:动态惩罚 `current_w_stem` (-1.0→-20.0)
     - **树叶碰撞 (Leaf)**:轻微惩罚 `-0.5 * (1.0 + vel)` (允许轻触)
  8. **🆕 动作变化率惩罚 (Action Rate)**:
     - 惩罚动作突变:-0.05 * ||action_diff||

- **🆕 Stage 2 特有奖励 (RewardManagerStage2.py)**:
   9. **扰动测试奖励 (Perturbation Test)**:
      - 当接触力 ≥ 1.0N 时,启动扰动测试
      - 施加 2-5N 随机外力,测试 0.5秒
      - **滑移 < 1cm**: 奖励 +10.0 (抗干扰成功)
      - **滑移 ≥ 1cm**: 惩罚 -5.0 (抓取不稳)
      - 模拟真实环境中的风吹、拉扯等干扰
   10. **加强速度惩罚 (Strengthened Impact Penalty)**:
       - 公式: `reward = -6.0 * (exp(5.0 * max(0, speed - 0.3)) - 1.0)`
       - 0.5 m/s 时惩罚 -10.3,有效抵消最大接触奖励 9.0
       - 防止智能体学会"冲击式"抓取,鼓励温柔接触

#### **control_manager.py**
- **作用**: 控制管理器
- **功能**:
  - 处理高层动作到底层控制的转换
  - **🆕 精细碰撞检测**:
    - 区分 `plant_contacts` (树干)和 `leaf_contacts` (树叶)
    - 通过 body 名称前缀识别 (`plant_stem*` / `plant_leaf*`)
  - **🆕 关节空间控制**:
    - 直接输出6维关节角度增量
    - 实时裁剪到关节限位
  - 检测接触信息(手-物体、底盘-环境)
  - 实现安全约束检查
  - 控制信号滤波和限制(LPF低通滤波)

#### **randomization_manager.py**
- **作用**: 随机化管理器
- **功能**:
  - **🆕 农业场景随机化**:
    - `randomize_plants()`: 随机生成植株(树干和树叶)位置
    - `randomize_fruit_and_occlusion()`: 随机果实位置,确保在前视野内
  - 物体属性随机化:质量、摩擦系数、尺寸
  - 环境随机化:光照、纹理
  - 物理参数随机化:重力、阻尼
  - 初始状态随机化:物体位置、机器人姿态
  - 支持域随机化以提高 Sim2Real 迁移性能

#### **render_manager.py**
- **作用**: 渲染管理器
- **功能**:
  - 处理深度图像预处理
  - 添加观测噪声(模拟真实传感器)
  - 图像掩码处理
  - 深度归一化和可视化

#### **constants.py**
- **作用**: 常量定义
- **功能**:
  - 定义全局控制变量(键盘输入)
  - 键盘回调函数:手动控制模式
  - 暂停/继续标志

### 2.3 **gym_dcmm/agents/** - 机器人智能体目录

#### **MujocoDcmm.py**
- **作用**: MuJoCo 机器人类
- **功能**:
  - 封装 MuJoCo 模型的加载和初始化
  - 实现机器人的运动学和动力学计算
  - 提供关节控制接口
  - 集成 PID 控制器(底盘驱动、机械臂、灵巧手)
  - **🆕 关节空间控制**:移除IK求解器依赖
  - 处理机械臂和灵巧手的协调控制
  - 获取机器人状态信息(位置、速度、力矩等)

### 2.4 **gym_dcmm/algs/** - 算法实现目录

#### **🆕 算法目录结构**
```
gym_dcmm/algs/ppo_dcmm/
├── stage1/                # Stage 1 (Tracking) 算法
│   ├── PPO_Stage1.py     # Stage 1 PPO 实现
│   └── ModelsStage1.py   # Stage 1 网络模型(CNN+MLP)
├── stage2/                # Stage 2 (Catching) 算法
│   ├── PPO_Stage2.py     # Stage 2 PPO 实现
│   └── ModelsStage2.py   # Stage 2 网络模型
└── (共享组件)
    ├── experience.py      # 经验缓冲
    └── utils.py           # 工具函数
```

#### **gym_dcmm/algs/ppo_dcmm/** - PPO 算法子目录

##### **stage1/PPO_Stage1.py**
- **作用**: Stage 1 (Tracking) 任务的 PPO 实现
- **功能**:
  - 实现第一阶段训练(追踪任务)
  - 处理视觉-状态融合输入
  - 训练底盘和机械臂的联合策略
  - 支持观测和奖励归一化
  - **🆕 课程学习集成**:
    - 在 `play_steps()` 开始时调用 `env.set_global_step()`
    - 同步全局步数到环境,触发课程调度
  - 保存和加载检查点
  - 实现测试模式

##### **stage2/PPO_Stage2.py**
- **作用**: Stage 2 (Catching) 任务的 PPO 实现
- **功能**:
  - 实现第二阶段训练(抓取任务)
  - **加载并冻结**第一阶段训练的追踪模型
  - 仅训练灵巧手的抓取策略
  - 底盘和机械臂由预训练模型控制
  - 实现策略解耦和模块化训练

##### **stage1/ModelsStage1.py**
- **作用**: Stage 1 (Tracking) 任务的神经网络模型
- **功能**:
  - **CNNBase**: 深度图像特征提取(卷积神经网络)
    - 3层卷积:32→64→64通道
    - 全连接层输出256维特征
  - **MLP**: 多层感知机(状态处理)
    - ELU 激活函数
    - 正交初始化
  - **ActorCritic**: 演员-评论家架构
    - 共享特征提取层
    - 独立的策略头和价值头
    - 支持多模态输入(图像+状态)

##### **stage2/ModelsStage2.py**
- **作用**: Stage 2 (Catching) 任务的神经网络模型
- **功能**:
  - **MLP**: 状态编码网络
  - **ActorCritic**: 
    - actor_mlp_t: 追踪部分的演员网络(冻结)
    - actor_mlp_c: 抓取部分的演员网络(可训练)
    - 独立的价值网络
    - 分离的策略输出(mu_t 用于追踪,mu_c 用于抓取)

##### **experience.py**
- **作用**: 经验缓冲区
- **功能**:
  - 存储轨迹数据:状态、动作、奖励、价值等
  - 计算 GAE (Generalized Advantage Estimation)
  - 生成训练批次
  - 支持多环境并行
  - **🆕 内存优化 (2025-12)**:
    - 深度观测使用 `uint8` 存储 (1 byte/pixel)
    - 状态观测使用 `float32` 存储 (4 bytes/element)
    - 训练时自动转换 `uint8` → `float32`
    - **节省 75% 深度缓冲显存**: 220MB → 55MB (典型配置)

##### **utils.py**
- **作用**: PPO 算法工具函数
- **功能**:
  - AverageScalarMeter: 标量指标平均
  - RunningMeanStd: 运行时均值和标准差计算(用于归一化)
  - 其他辅助函数

### 2.5 **gym_dcmm/utils/** - 工具函数目录

#### **util.py**
- **作用**: 通用工具函数
- **功能**:
  - omegaconf_to_dict: 配置对象转换
  - get_total_dimension: 计算观测维度
  - DynamicDelayBuffer: 动作延迟缓冲器(模拟真实延迟)
  - 其他数据处理函数

#### **pid.py**
- **作用**: PID 控制器实现
- **功能**:
  - 标准 PID 控制算法
  - 支持多维输入
  - 可配置增益和限制
  - 积分抗饱和

#### **quat_utils.py**
- **作用**: 四元数工具函数
- **功能**:
  - 四元数旋转向量
  - 四元数乘法、求逆
  - 姿态转换
  - 角度计算

---

## 3️⃣ **assets/** - 资源文件目录

### 3.1 **assets/meshes/** - 3D 网格模型目录
- **作用**: 存储机器人和物体的 3D 模型文件
- **功能**: 提供 MuJoCo 仿真的视觉模型

#### 主要文件分类:

**机器人底盘相关**:
- `base_link.STL`, `simple_base_link.stl`: 底盘基座
- `Wheel1_Link.STL ~ Wheel4_Link.STL`: 四个车轮
- `wheel_1_1.stl ~ wheel_4_1.stl`: 车轮备选模型
- `Suspension1_Link.STL ~ Suspension4_Link.STL`: 悬挂系统

**机械臂相关**:
- `Arm_base_Link.STL`: 机械臂基座
- `a1_Link.STL ~ a5_Link.STL`: 机械臂连杆(5个)
- `link_0.0.STL ~ link_3.0.STL`: xArm6 连杆
- `link1.stl ~ link6.stl`: 关节连接件

**灵巧手相关**:
- `hand_link.STL`, `hand_link_left.STL`: 手掌主体
- `palm_lower.stl`: 手掌下部
- `mcp_joint.stl`: 掌指关节
- `pip.stl`, `dip.stl`: 近端/远端指间关节
- `fingertip.stl`, `modified_tip.STL`: 指尖
- `thumb_pip.stl`, `thumb_dip.stl`, `thumb_fingertip.stl`: 拇指部件

**🆕 农业场景物体**:
- 果实模型(mocap静态物体)
- 植株树干和树叶模型
- 地面、围栏等环境物体

### 3.2 **assets/urdf/** - URDF/XML 模型描述目录

#### **x1_xarm6_leap_right_object.xml**
- **作用**: 完整机器人系统的 MuJoCo XML 描述(训练用物体集)
- **功能**:
  - 定义机器人的运动学树结构
  - 指定关节类型、限制、驱动方式
  - 引用网格文件
  - 定义传感器、摄像头(顶部摄像头)
  - 设置碰撞检测
  - **🆕 农业场景定义**:
    - mocap果实定义
    - 植株生成逻辑(树干/树叶)
    - 视野约束(果实和植株仅在前方)

### 3.3 **assets/models/** - 预训练模型目录

#### **track.pth**
- **作用**: 第一阶段(追踪)预训练模型
- **功能**:
  - 包含训练好的追踪策略网络权重
  - 用于两阶段训练的第二阶段
  - 可直接加载进行追踪任务推理

#### **catch_two_stage.pth**
- **作用**: 第二阶段(抓取)预训练模型
- **功能**:
  - 包含训练好的抓取策略网络权重
  - 配合 track.pth 实现完整的移动抓取
  - 用于测试和演示

---

## 🔄 核心工作流程

### 训练流程

1. **配置加载** (`configs/config.yaml`)
   - Hydra 加载主配置和训练配置
   - 设置设备、随机种子、任务类型

2. **环境创建** (`gym_dcmm/envs/DcmmVecEnv.py`)
   - 初始化 MuJoCo 模型
   - 创建向量化环境(多个并行环境)
   - 配置观测空间和动作空间
   - **🆕 初始化课程学习参数**

3. **智能体初始化**
   - **追踪任务**: `PPO_Track` + `models_track.ActorCritic`
   - **抓取任务**: `PPO_Catch_TwoStage` + `models_catch.ActorCritic`

4. **🆕 课程学习训练循环**
   - **0-2M步 (幼儿园阶段)**:
     - 树干惩罚:-1.0 (轻微提醒)
     - 朝向要求:1次方(宽松)
     - 鼓励探索,建立基本能力
   - **2M-6M步 (小学阶段)**:
     - 树干惩罚:-1.0 → -20.0 (线性增长)
     - 朝向要求:1次方 → 4次方(渐进严格)
     - 学会规避障碍,精确定位
   - **>6M步 (职业阶段)**:
     - 树干惩罚:-20.0 (零容忍)
     - 朝向要求:4次方(极严格)
     - 精准作业,优雅动作
   - 收集经验 → 计算优势函数 → 更新策略 → 记录指标
   - 定期保存检查点到 `outputs/`
   - 同步数据到 WandB

5. **测试评估**
   - 加载预训练模型
   - 在测试环境中评估性能
   - 生成可视化结果

### 🆕 动态课程学习机制

```
训练开始 (step=0)
    ↓
play_steps() 调用 env.set_global_step(current_step)
    ↓
env.update_curriculum_difficulty()
    ├─ 计算 difficulty = step / 6M (0.0 → 1.0)
    ├─ current_w_stem = -1.0 + (-20.0 - (-1.0)) * difficulty
    └─ current_orient_power = 1.0 + (4.0 - 1.0) * difficulty
    ↓
reward_manager.compute_reward() 使用动态参数
    ├─ 树干碰撞惩罚 = current_w_stem
    └─ 朝向奖励 = alignment^current_orient_power
    ↓
智能体逐渐从"探索"变为"精准"
```

### 两阶段训练流程

**第一阶段 - 追踪 (Tracking)**:
```
train_DCMM.py (task=Tracking)
    ↓
PPO_Track
    ↓
models_track.ActorCritic (CNN + MLP)
    ↓
训练底盘 + 机械臂 接近目标(果实)
    ├─ 课程学习:逐步提高避障和朝向要求
    └─ 固定手掌为张开姿态
    ↓
保存 track.pth
```

**第二阶段 - 抓取 (Catching)**:
```
train_DCMM.py (task=Catching_TwoStage)
    ↓
PPO_Catch_TwoStage
    ↓
加载 track.pth (冻结)
    ↓
训练灵巧手抓取策略
    ↓
保存 catch_two_stage.pth
```

---

## 🔑 关键技术点

### 1. **模块化设计**
- 将环境功能分解为多个管理器(观测、奖励、随机化、控制、渲染)
- 便于维护和扩展

### 2. **两阶段训练**
- 分解复杂任务,提高训练效率
- 第一阶段专注于导航,第二阶段专注于操作

### 3. **🆕 动态课程学习**
- **渐进式难度调节**:从宽松到严格,避免"过早失败"
- **双重调节机制**:
  - 碰撞惩罚权重:控制探索vs安全的平衡
  - 朝向精度要求:控制粗略vs精准的过渡
- **自动调度**:基于全局步数自动插值,无需手动干预

### 4. **🆕 精细化奖励设计**
- **分层碰撞检测**:区分树干(刚性,严禁)和树叶(柔性,容忍)
- **归一化奖励**:使用 tanh/exp 防止奖励爆炸
- **速度感知惩罚**:鼓励温柔接触和平滑动作
- **多目标平衡**:到达、定位、朝向、接触、平滑性综合优化

### 5. **域随机化**
- 物理参数、物体属性、观测噪声的随机化
- 提高 Sim2Real 迁移性能
- **🆕 农业场景随机化**:植株位置、果实位置、遮挡程度

### 6. **🆕 关节空间控制**
- 直接输出关节角度增量,避免IK不稳定
- 实时限位裁剪,保证安全性
- 更适合学习精细操作

### 7. **多模态输入**
- 融合视觉(深度图像)和状态(关节位置、速度)信息
- 提供更丰富的感知
- **🆕 鲁棒性增强**:噪声+缺失模拟

### 8. **动作延迟模拟**
- DynamicDelayBuffer 模拟真实机器人的执行延迟
- 增强策略鲁棒性

---

## 📊 数据流图

```
MuJoCo 仿真环境 (农业场景)
    ↓
观测管理器 → [状态向量 + 深度图像(带噪声)]
    ↓
神经网络 (CNN + MLP)
    ↓
策略输出 (动作分布)
    ↓
控制管理器 → [底盘速度 + 关节角度增量]
    ↓
延迟缓冲器 → 执行动作
    ↓
MuJoCo 仿真步进
    ↓
碰撞检测 → [树干/树叶分离]
    ↓
课程调度器 → [动态调整惩罚和要求]
    ↓
奖励管理器 → 计算奖励
    ↓
经验缓冲 → PPO 更新
```

---

## 🎯 总结

本项目是一个完整的强化学习农业采摘系统,具有以下特点:

- **完整性**: 从环境搭建、算法实现到训练评估的完整流程
- **模块化**: 清晰的代码组织和职责分离
- **🆕 文件结构分离**: Stage 1 和 Stage 2 完全独立,便于调试和修改
- **🆕 智能化**: 动态课程学习自动调整训练难度
- **🆕 精细化**: 分层碰撞检测、速度感知奖励、关节空间控制
- **可扩展**: 易于添加新场景、新任务、新算法
- **实用性**: 面向真实机器人部署,包含域随机化和延迟模拟
- **可复现**: 详细的配置管理和实验跟踪

### 🆕 最新改进总结

1. **任务转型**: 从通用抓取 → 农业果实采摘(静态目标)
2. **🆕 文件系统重构 (2025-12)**:
   - 创建 `stage1/` 和 `stage2/` 独立目录
   - 标准化类名: `*Stage1` / `*Stage2`
   - 独立训练脚本: `train_stage1.py` / `train_stage2.py`
   - 独立配置: `config_stage1.yaml` / `config_stage2.yaml`
3. **课程学习**: 实现"幼儿园→小学→职业"三阶段渐进训练
4. **奖励重构**: 8种奖励分量,动态参数调节
5. **控制升级**: IK控制 → 关节空间直接控制
6. **碰撞细化**: 树干/树叶分离检测和差异化惩罚
7. **视觉增强**: 深度噪声+像素缺失模拟
8. **🆕 Stage 2 优化 (2025-12)**:
   - **内存效率**: `experience.py` 使用 `uint8` 存储深度,节省 75% 显存
   - **扰动验证**: `RewardManagerStage2` 新增外力测试 (2-5N),验证抓取稳定性
   - **速度控制**: 惩罚系数 `-6.0`,0.5m/s 下惩罚 -10.3 有效抵消接触奖励

通过本文档,您应该能够全面理解项目的每个部分及其作用,特别是最新的文件系统重构、课程学习、奖励函数改进以及 Stage 2 的内存优化和抓取质量提升,为进一步开发和使用提供指导。
