# Catch It 项目架构详细说明文档

## 📋 项目概述

**Catch It** 是一个基于强化学习的移动抓取项目，采用两阶段训练方法训练移动操作机器人（配备移动底盘、6自由度机械臂和灵巧手）自主追踪并抓取物体。项目使用 MuJoCo 物理引擎进行仿真，采用 PPO (Proximal Policy Optimization) 算法进行训练。

---

## 📁 根目录文件说明

### 1. **README.md**
- **作用**: 项目说明文档
- **功能**: 
  - 介绍项目核心特性和两阶段训练框架
  - 提供安装指南和使用说明
  - 说明追踪(Tracking)和抓取(Catching)两个训练阶段的目标和机制

### 2. **setup.py**
- **作用**: Python 包安装脚本
- **功能**:
  - 定义项目名称为 `gym_dcmm` (Gymnasium environment for Dexterous Catch with Mobile Manipulation)
  - 指定依赖包：`gymnasium==0.29.1` 和 `mujoco>=3.0.0`
  - 配置包的安装参数和元信息
  - 支持通过 `pip install -e .` 进行开发模式安装

### 3. **requirements.txt**
- **作用**: Python 依赖包列表
- **功能**: 列出项目所需的所有第三方库及版本，包括：
  - **数值计算**: numpy, scipy
  - **图像处理**: opencv-python, imageio
  - **机器人学**: spatialmath-python, numpy-quaternion
  - **优化求解**: qpsolvers, quadprog
  - **配置管理**: PyYAML, omegaconf, hydra-core
  - **实验记录**: wandb, tensorboardX
  - **其他工具**: termcolor, decorators, seaborn

### 4. **train_DCMM.py**
- **作用**: 主训练脚本
- **功能**:
  - 使用 Hydra 框架管理配置
  - 根据任务类型（Tracking/Catching_TwoStage/Catching_OneStage）创建相应的环境
  - 初始化对应的 PPO 智能体
  - 支持训练和测试模式切换
  - 集成 WandB 进行实验跟踪和可视化
  - 自动保存模型检查点到 `outputs/` 目录

### 5. **test_env.py**
- **作用**: 环境测试脚本
- **功能**:
  - 验证 DcmmVecEnv 环境是否正常初始化
  - 测试环境的 reset 和 step 功能
  - 检查观测空间（深度图像、状态向量）的形状和数值范围
  - 用于调试环境配置和接口

### 6. **MUJOCO_LOG.TXT**
- **作用**: MuJoCo 仿真日志文件
- **功能**: 记录 MuJoCo 物理引擎的运行日志、警告和错误信息

### 7. **项目架构分析.md**
- **作用**: 项目架构分析文档（已存在）
- **功能**: 提供项目结构的简要分析

---

## 📂 主要目录结构详解

## 1️⃣ **configs/** - 配置文件目录

### 1.1 **configs/config.yaml**
- **作用**: 主配置文件
- **功能**:
  - 定义实验基本参数：随机种子、设备配置（CPU/GPU）
  - 设置任务类型：Tracking（追踪）、Catching_TwoStage（两阶段抓取）、Catching_OneStage（单阶段抓取）
  - 配置环境参数：并行环境数量、是否使用视觉输入、是否显示可视化
  - 指定检查点路径：预训练模型的加载路径
  - WandB 配置：实验跟踪、项目名称、实体名称、运行模式
  - Hydra 框架配置：输出目录设置

### 1.2 **configs/env/** - 环境配置子目录

#### **configs/env/DcmmCfg.py**
- **作用**: 环境核心配置文件
- **功能**:
  - **路径定义**: 
    - ASSET_PATH: 资源文件根路径
    - XML 模型路径：机器人和场景的 XML 描述文件
  - **初始状态配置**:
    - arm_joints: 机械臂初始关节角度（6个关节）
    - hand_joints: 灵巧手初始关节角度（16个关节）
  - **奖励权重**:
    - r_base_pos: 底盘位置奖励
    - r_ee_pos: 末端执行器位置奖励
    - r_precision: 精确度奖励
    - r_orient: 方向奖励
    - r_touch: 接触奖励（追踪和抓取任务不同）
    - r_constraint: 约束奖励
    - r_stability: 稳定性奖励
    - r_ctrl: 控制惩罚（底盘、机械臂、灵巧手分别设置）
    - r_collision: 碰撞惩罚
    - r_finger_approach: 手指接近奖励
    - r_force_closure: 力闭合奖励
  - **任务参数**:
    - distance_thresh: 从追踪阶段切换到抓取阶段的距离阈值
  - **PID 控制参数**: 底盘驱动的 PID 增益和限制

### 1.3 **configs/train/** - 训练配置子目录

#### **configs/train/DcmmPPO.yaml**
- **作用**: PPO 算法训练参数配置
- **功能**:
  - **网络架构**:
    - MLP 隐藏层单元数：[256, 128]
    - 是否使用独立的价值网络
  - **PPO 超参数**:
    - 学习率：5e-4
    - 折扣因子 gamma：0.99
    - GAE lambda (tau)：0.95
    - Clip 范围：0.2
    - 熵系数：0.0
    - 价值函数系数：4
  - **训练设置**:
    - 水平长度 (horizon_length)：64
    - Mini-batch 大小：512
    - Mini-epochs：6
    - 最大训练步数：25,000,000
  - **动作归一化参数**:
    - action_track_denorm: 追踪任务动作反归一化 [底盘1.5, 末端执行器0.025, 手部0.01]
    - action_catch_denorm: 抓取任务动作反归一化 [1.5, 0.025, 0.15]
  - **图像配置**:
    - 图像尺寸：[224, 224]
    - 帧堆叠数：2

---

## 2️⃣ **gym_dcmm/** - 核心环境包

这是项目的核心模块，包含环境定义、智能体控制、算法实现和工具函数。

### 2.1 **gym_dcmm/__init__.py**
- **作用**: 包初始化文件
- **功能**:
  - 注册 Gymnasium 环境 `gym_dcmm/DcmmVecWorld-v0`
  - 指定入口点为 `gym_dcmm.envs:DcmmVecEnv`

### 2.2 **gym_dcmm/envs/** - 环境实现目录

#### **DcmmVecEnv.py**
- **作用**: 主环境类（向量化环境）
- **功能**:
  - 实现 Gymnasium 接口：reset(), step(), render()
  - 支持两种任务模式：Tracking（追踪）和 Catching（抓取）
  - 提供多种渲染模式：rgb_array、depth_array、depth_rgb_array
  - 集成各个管理器模块：观测、奖励、随机化、控制、渲染
  - 定义观测空间和动作空间
  - 处理动作延迟缓冲（模拟真实机器人延迟）

#### **observation_manager.py**
- **作用**: 观测管理器
- **功能**:
  - 收集机器人状态：底盘速度、机械臂关节状态、灵巧手关节状态
  - 处理目标物体信息：位置、速度
  - 计算相对位置和方向
  - 处理深度图像观测
  - 坐标系转换（世界坐标系 ↔ 基座坐标系）

#### **reward_manager.py**
- **作用**: 奖励计算管理器
- **功能**:
  - 实现复杂的奖励函数设计
  - 计算各种奖励分量：位置、方向、接触、稳定性等
  - 根据任务类型调整奖励权重
  - 检测成功条件（抓取成功、物体掉落等）
  - 动作平滑性惩罚

#### **control_manager.py**
- **作用**: 控制管理器
- **功能**:
  - 处理高层动作到底层控制的转换
  - 检测接触信息（手-物体、底盘-环境）
  - 实现安全约束检查
  - 控制信号滤波和限制

#### **randomization_manager.py**
- **作用**: 随机化管理器
- **功能**:
  - 物体属性随机化：质量、摩擦系数、尺寸
  - 环境随机化：光照、纹理
  - 物理参数随机化：重力、阻尼
  - 初始状态随机化：物体位置、机器人姿态
  - 支持域随机化以提高 Sim2Real 迁移性能

#### **render_manager.py**
- **作用**: 渲染管理器
- **功能**:
  - 处理深度图像预处理
  - 添加观测噪声（模拟真实传感器）
  - 图像掩码处理
  - 深度归一化和可视化

#### **constants.py**
- **作用**: 常量定义
- **功能**:
  - 定义全局控制变量（键盘输入）
  - 键盘回调函数：手动控制模式
  - 暂停/继续标志

### 2.3 **gym_dcmm/agents/** - 机器人智能体目录

#### **MujocoDcmm.py**
- **作用**: MuJoCo 机器人类
- **功能**:
  - 封装 MuJoCo 模型的加载和初始化
  - 实现机器人的运动学和动力学计算
  - 提供关节控制接口
  - 集成 PID 控制器（底盘驱动）
  - 实现逆运动学（IK）求解器接口
  - 处理机械臂和灵巧手的协调控制
  - 获取机器人状态信息（位置、速度、力矩等）

### 2.4 **gym_dcmm/algs/** - 算法实现目录

#### **gym_dcmm/algs/ppo_dcmm/** - PPO 算法子目录

##### **ppo_dcmm_track.py**
- **作用**: 追踪任务的 PPO 实现
- **功能**:
  - 实现第一阶段训练（追踪任务）
  - 处理视觉-状态融合输入
  - 训练底盘和机械臂的联合策略
  - 支持观测和奖励归一化
  - 保存和加载检查点
  - 实现测试模式

##### **ppo_dcmm_catch_two_stage.py**
- **作用**: 两阶段抓取任务的 PPO 实现
- **功能**:
  - 实现第二阶段训练（抓取任务）
  - **加载并冻结**第一阶段训练的追踪模型
  - 仅训练灵巧手的抓取策略
  - 底盘和机械臂由预训练模型控制
  - 实现策略解耦和模块化训练

##### **ppo_dcmm_catch_one_stage.py**
- **作用**: 单阶段抓取任务的 PPO 实现
- **功能**:
  - 端到端训练完整的移动抓取策略
  - 同时优化底盘、机械臂和灵巧手
  - 适用于对比实验

##### **models_track.py**
- **作用**: 追踪任务的神经网络模型
- **功能**:
  - **CNNBase**: 深度图像特征提取（卷积神经网络）
    - 3层卷积：32→64→32通道
    - 全连接层输出256维特征
  - **MLP**: 多层感知机（状态处理）
    - ELU 激活函数
    - 正交初始化
  - **ActorCritic**: 演员-评论家架构
    - 共享特征提取层
    - 独立的策略头和价值头
    - 支持多模态输入（图像+状态）

##### **models_catch.py**
- **作用**: 抓取任务的神经网络模型
- **功能**:
  - **MLP**: 状态编码网络
  - **ActorCritic**: 
    - actor_mlp_t: 追踪部分的演员网络（冻结）
    - actor_mlp_c: 抓取部分的演员网络（可训练）
    - 独立的价值网络
    - 分离的策略输出（mu_t 用于追踪，mu_c 用于抓取）

##### **experience.py**
- **作用**: 经验缓冲区
- **功能**:
  - 存储轨迹数据：状态、动作、奖励、价值等
  - 计算 GAE (Generalized Advantage Estimation)
  - 生成训练批次
  - 支持多环境并行

##### **utils.py**
- **作用**: PPO 算法工具函数
- **功能**:
  - AverageScalarMeter: 标量指标平均
  - RunningMeanStd: 运行时均值和标准差计算（用于归一化）
  - 其他辅助函数

### 2.5 **gym_dcmm/utils/** - 工具函数目录

#### **util.py**
- **作用**: 通用工具函数
- **功能**:
  - omegaconf_to_dict: 配置对象转换
  - get_total_dimension: 计算观测维度
  - DynamicDelayBuffer: 动作延迟缓冲器（模拟真实延迟）
  - 其他数据处理函数

#### **pid.py**
- **作用**: PID 控制器实现
- **功能**:
  - 标准 PID 控制算法
  - 支持多维输入
  - 可配置增益和限制
  - 积分抗饱和

#### **quat_utils.py**
- **作用**: 四元数工具函数
- **功能**:
  - 四元数旋转向量
  - 四元数乘法、求逆
  - 姿态转换
  - 角度计算

#### **gym_dcmm/utils/ik_pkg/** - 逆运动学包

##### **ik_arm.py**
- **作用**: 机械臂逆运动学求解器
- **功能**:
  - 给定末端执行器目标位置和姿态，计算关节角度
  - 基于雅可比矩阵的数值解法
  - 关节限制处理

##### **ik_base.py**
- **作用**: 移动底盘逆运动学
- **功能**:
  - 差分驱动运动学
  - 路径规划辅助
  - 速度映射

---

## 3️⃣ **assets/** - 资源文件目录

### 3.1 **assets/meshes/** - 3D 网格模型目录
- **作用**: 存储机器人和物体的 3D 模型文件
- **功能**: 提供 MuJoCo 仿真的视觉模型

#### 主要文件分类：

**机器人底盘相关**:
- `base_link.STL`, `simple_base_link.stl`: 底盘基座
- `Wheel1_Link.STL ~ Wheel4_Link.STL`: 四个车轮
- `wheel_1_1.stl ~ wheel_4_1.stl`: 车轮备选模型
- `Suspension1_Link.STL ~ Suspension4_Link.STL`: 悬挂系统

**机械臂相关**:
- `Arm_base_Link.STL`: 机械臂基座
- `a1_Link.STL ~ a5_Link.STL`: 机械臂连杆（5个）
- `link_0.0.STL ~ link_3.0.STL`: xArm6 连杆
- `link1.stl ~ link6.stl`: 关节连接件

**灵巧手相关**:
- `hand_link.STL`, `hand_link_left.STL`: 手掌主体
- `palm_lower.stl`: 手掌下部
- `mcp_joint.stl`: 掌指关节
- `pip.stl`, `dip.stl`: 近端/远端指间关节
- `fingertip.stl`, `modified_tip.STL`: 指尖
- `thumb_pip.stl`, `thumb_dip.stl`, `thumb_fingertip.stl`: 拇指部件

**抓取物体**:
- `bottle.stl`: 瓶子
- `bowl.stl`: 碗
- `cup.stl`: 杯子
- `bread.stl`: 面包
- `lemon.stl`: 柠檬
- `milk.stl`: 牛奶盒
- `winnercup.stl`: 奖杯
- `can.stl`: 易拉罐
- `cereal.stl`: 麦片盒
- 每个物体还有对应的 `.obj`, `.mtl`, `.msh` 文件（不同格式）

**环境物体**:
- `table_top.STL`, `table_leg.STL`: 桌子
- `cube.obj`, `sphere8.obj`, `cylinder.obj`: 基本几何体
- `car.STL`, `benchy.stl`, `ufo.stl`: 其他测试物体

### 3.2 **assets/urdf/** - URDF/XML 模型描述目录

#### **x1_xarm6_leap_right_object.xml**
- **作用**: 完整机器人系统的 MuJoCo XML 描述（训练用物体集）
- **功能**:
  - 定义机器人的运动学树结构
  - 指定关节类型、限制、驱动方式
  - 引用网格文件
  - 定义传感器、摄像头
  - 设置碰撞检测
  - 包含训练用的物体集合

#### **x1_xarm6_leap_right_unseen_object.xml**
- **作用**: 完整机器人系统的 MuJoCo XML 描述（测试用未见物体集）
- **功能**: 与上述相同，但包含训练时未见过的物体，用于泛化能力测试

#### **xarm6_right.xml**
- **作用**: 仅包含 xArm6 机械臂的 XML 描述
- **功能**: 
  - 用于单独的机械臂逆运动学计算
  - 提供独立的机械臂仿真环境

### 3.3 **assets/models/** - 预训练模型目录

#### **track.pth**
- **作用**: 第一阶段（追踪）预训练模型
- **功能**:
  - 包含训练好的追踪策略网络权重
  - 用于两阶段训练的第二阶段
  - 可直接加载进行追踪任务推理

#### **catch_two_stage.pth**
- **作用**: 第二阶段（抓取）预训练模型
- **功能**:
  - 包含训练好的抓取策略网络权重
  - 配合 track.pth 实现完整的移动抓取
  - 用于测试和演示

### 3.4 **assets/objects/** - 物体模型目录
- **作用**: 存储可抓取物体的独立模型
- **功能**: 与 meshes/ 中的物体文件对应，提供物体的单独描述

**包含的物体**:
- `bottle.stl`: 瓶子
- `bowl.stl`: 碗
- `bread.stl`: 面包
- `cup.stl`: 杯子
- `lemon.stl`: 柠檬
- `milk.stl`: 牛奶盒
- `winnercup.stl`: 奖杯

### 3.5 **assets/textures/** - 纹理文件目录
- **作用**: 存储材质和纹理贴图（当前为空）
- **功能**: 用于渲染时的视觉增强（可选）

### 3.6 **assets/media/** - 媒体文件目录

#### **assets/media/imgs/**
- **作用**: 存储图像文件
- **功能**: 保存实验结果截图、可视化图像

#### **assets/media/videos/**
- **作用**: 存储视频文件
- **功能**: 保存训练/测试过程的视频记录、演示动画

---

## 4️⃣ **outputs/** - 输出目录

### **outputs/Dcmm/**
- **作用**: 训练输出的根目录
- **功能**:
  - 按时间戳组织训练结果：`YYYY-MM-DD/HH:MM:SS/`
  - 存储模型检查点（.pth 文件）
  - 保存 TensorBoard 日志
  - 记录训练配置快照
  - 存储评估结果

---

## 5️⃣ **wandb/** - WandB 目录

- **作用**: Weights & Biases 实验跟踪目录
- **功能**:
  - 存储 WandB 本地缓存
  - 记录实验元数据
  - 同步实验数据到云端

**主要文件**:
- `settings`: WandB 配置
- `debug-internal.log`: 内部调试日志
- `debug.log`: 用户调试日志
- `debug-cli.cle.log`: 命令行调试日志
- `latest-run`: 最新运行的符号链接

---

## 6️⃣ **gym_dcmm.egg-info/** - 包元数据目录

- **作用**: Python 包安装信息（通过 setup.py 生成）
- **功能**: pip 包管理使用的元数据

**主要文件**:
- `PKG-INFO`: 包基本信息
- `SOURCES.txt`: 源文件列表
- `requires.txt`: 依赖列表
- `top_level.txt`: 顶级模块名
- `dependency_links.txt`: 依赖链接
- `not-zip-safe`: 标记包不能压缩

---

## 7️⃣ **项目技术文档/** - 技术文档目录

- **作用**: 存储详细的技术文档和设计说明
- **功能**: 项目开发和维护的参考资料

---

## 🔄 核心工作流程

### 训练流程

1. **配置加载** (`configs/config.yaml`)
   - Hydra 加载主配置和训练配置
   - 设置设备、随机种子、任务类型

2. **环境创建** (`gym_dcmm/envs/DcmmVecEnv.py`)
   - 初始化 MuJoCo 模型
   - 创建向量化环境（多个并行环境）
   - 配置观测空间和动作空间

3. **智能体初始化**
   - **追踪任务**: `PPO_Track` + `models_track.ActorCritic`
   - **抓取任务**: `PPO_Catch_TwoStage` + `models_catch.ActorCritic`

4. **训练循环**
   - 收集经验 → 计算优势函数 → 更新策略 → 记录指标
   - 定期保存检查点到 `outputs/`
   - 同步数据到 WandB

5. **测试评估**
   - 加载预训练模型
   - 在测试环境中评估性能
   - 生成可视化结果

### 两阶段训练流程

**第一阶段 - 追踪 (Tracking)**:
```
train_DCMM.py (task=Tracking)
    ↓
PPO_Track
    ↓
models_track.ActorCritic (CNN + MLP)
    ↓
训练底盘 + 机械臂 接近物体
    ↓
保存 track.pth
```

**第二阶段 - 抓取 (Catching)**:
```
train_DCMM.py (task=Catching_TwoStage)
    ↓
PPO_Catch_TwoStage
    ↓
加载 track.pth (冻结)
    ↓
训练灵巧手抓取策略
    ↓
保存 catch_two_stage.pth
```

---

## 🔑 关键技术点

### 1. **模块化设计**
- 将环境功能分解为多个管理器（观测、奖励、随机化、控制、渲染）
- 便于维护和扩展

### 2. **两阶段训练**
- 分解复杂任务，提高训练效率
- 第一阶段专注于导航，第二阶段专注于操作

### 3. **域随机化**
- 物理参数、物体属性、观测噪声的随机化
- 提高 Sim2Real 迁移性能

### 4. **多模态输入**
- 融合视觉（深度图像）和状态（关节位置、速度）信息
- 提供更丰富的感知

### 5. **动作延迟模拟**
- DynamicDelayBuffer 模拟真实机器人的执行延迟
- 增强策略鲁棒性

---

## 📊 数据流图

```
MuJoCo 仿真环境
    ↓
观测管理器 → [状态向量 + 深度图像]
    ↓
神经网络 (CNN + MLP)
    ↓
策略输出 (动作分布)
    ↓
控制管理器 → [底盘速度 + 关节速度]
    ↓
延迟缓冲器 → 执行动作
    ↓
MuJoCo 仿真步进
    ↓
奖励管理器 → 计算奖励
    ↓
经验缓冲 → PPO 更新
```

---

## 🎯 总结

本项目是一个完整的强化学习移动抓取系统，具有以下特点：

- **完整性**: 从环境搭建、算法实现到训练评估的完整流程
- **模块化**: 清晰的代码组织和职责分离
- **可扩展**: 易于添加新物体、新任务、新算法
- **实用性**: 面向真实机器人部署，包含域随机化和延迟模拟
- **可复现**: 详细的配置管理和实验跟踪

通过本文档，您应该能够全面理解项目的每个部分及其作用，为进一步开发和使用提供指导。

