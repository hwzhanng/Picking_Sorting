基于代码逻辑和配置文件，以下是关于两个阶段训练分数的理论计算：

1. 第二阶段：抓取 (Catching)
这是最标准的强化学习任务，持续时间固定为 4 秒（100 个时间步）。

单步满分计算：
接近奖励 (Reaching): max(0, 5.0 - dist)。当完全接近时，约为 5.0。
接触奖励 (Touch): 抓取任务的权重是 0.1。当接触时，为 0.1。
力闭合/抬起奖励 (Force Closure): 权重是 5.0。当物体被抬起时，为 5.0。
手指距离惩罚 (Finger Approach): 接触时约为 0。
单步理论最大值: $5.0 + 0.1 + 5.0 = 10.1$。
总分计算 (100步)：
理论满分: $10.1 \times 100 \approx \mathbf{1010}$。
什么样的分数算好？
考虑到机器人需要约 0.5~1 秒的时间去接近和抓取物体，这期间只能获得接近奖励（约 2.5 分/步）。
假设前 25 步（1秒）在接近，后 75 步（3秒）稳定抓取并抬起：
接近阶段：$2.5 \times 25 \approx 62.5$
抓取阶段：$10.1 \times 75 \approx 757.5$
总计：$\approx 820$
结论：训练分数达到 800 分以上 就算是非常好的效果（意味着稳定抓取并抬起）。
2. 第一阶段：追踪 (Tracking)
这个阶段的奖励机制比较特殊，因为它会在手掌触碰到物体时判定为成功并立即结束回合。

单步满分计算：
接近奖励 (Reaching): 最大 5.0。
接触奖励 (Touch): 追踪任务的权重是 10.0。但只有在接触的那一瞬间（回合结束步）能拿到。
单步理论最大值: 悬停时为 5.0；接触瞬间为 15.0。
总分计算：
这就产生了一个悖论：
情况 A（快速成功）：如果机器人 0.5 秒就碰到了物体。
得分 $\approx 30$ (接近过程) + $15$ (接触瞬间) = 45 分。
情况 B（完美悬停）：如果机器人学会了无限接近物体但不触碰（悬停在表面），持续 4 秒。
得分 $\approx 5.0 \times 100 = \mathbf{500 分}$。
RL 的选择：由于强化学习追求总分最大化，在这个奖励设置下，Agent 可能会倾向于悬停在物体表面而不触碰，或者在最后时刻才触碰。
结论：
理论满分：500 分（一直悬停）。
什么样的分数算好？
如果你看到分数在 300 ~ 450 之间，说明 Agent 学会了非常精准地跟踪和悬停。
如果你看到分数很低（如 50），可能意味着它很快就完成了触摸任务（这也是一种成功，但在分数上不占优势）。
根据刚才的测试日志 (mean_rewards: 178)，Agent 目前处于“较长时间的跟踪/悬停”状态，这是一个不错的训练中间态。
总结参考值
Tracking (追踪): 目标 > 300 (代表稳定的跟踪能力)。
Catching (抓取): 目标 > 800 (代表稳定的抓取和抬起能力)。