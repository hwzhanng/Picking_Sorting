# 项目架构与核心逻辑详解

本文档旨在清晰、通俗地解释 "Catch It" 项目的核心文件关系、架构设计以及模型的训练流程和目标。

## 1. 项目核心架构与文件关系

本项目采用了模块化的设计，主要由 **配置 (Configs)**、**环境 (Environment)** 和 **算法 (Algorithms)** 三大部分组成。

### 1.1 核心文件概览

*   **入口脚本**: `train_DCMM.py`
    *   **作用**: 整个项目的“指挥官”。负责读取配置、初始化环境、选择对应的智能体（Agent），并启动训练或测试循环。
*   **配置文件**:
    *   `configs/config.yaml`: **总纲**。定义任务类型（Tracking 或 Catching）、训练参数（如环境数量 `num_envs`）、以及模型保存/加载路径。
    *   `configs/env/DcmmCfg.py`: **环境细节**。定义机器人的物理参数（PID增益）、奖励函数的权重（如触摸奖励、距离奖励）、以及 Sim2Real 相关的噪声和延迟参数。
*   **环境层 (Gymnasium)**:
    *   `gym_dcmm/envs/DcmmVecEnv.py`: **游戏规则制定者**。
        *   它定义了机器人能看到什么（观测空间 Observation）、能做什么（动作空间 Action）。
        *   它计算“分数”（奖励函数 Reward），告诉机器人做得好不好。
        *   它负责重置场景（Reset）和推演物理世界（Step）。
    *   `gym_dcmm/agents/MujocoDcmm.py`: **物理引擎接口**。
        *   直接与 MuJoCo 物理引擎交互。
        *   负责加载机器人模型（XML文件）、控制电机、读取传感器数据。
*   **算法层 (PPO)**:
    *   `gym_dcmm/algs/ppo_dcmm/ppo_dcmm_track.py`: **追踪教练**。负责训练第一阶段的追踪模型。
    *   `gym_dcmm/algs/ppo_dcmm/ppo_dcmm_catch_two_stage.py`: **抓取教练**。负责训练第二阶段的抓取模型，并负责加载和冻结第一阶段的模型。
    *   `models_track.py` / `models_catch.py`: **大脑结构**。定义了神经网络的具体层数和结构（Actor-Critic 网络）。

### 1.2 架构关系图

```mermaid
graph TD
    A[train_DCMM.py] -->|读取| B(configs/config.yaml)
    A -->|初始化| C[DcmmVecEnv (环境)]
    C -->|调用| D[MujocoDcmm (物理引擎)]
    C -->|读取| E(configs/env/DcmmCfg.py)
    A -->|选择| F{任务类型?}
    F -->|Tracking| G[PPO_Track (追踪教练)]
    F -->|Catching| H[PPO_Catch_TwoStage (抓取教练)]
    G -->|训练| I[Tracking Model (追踪大脑)]
    H -->|加载并冻结| I
    H -->|训练| J[Catching Model (抓取大脑)]
```

---

## 2. 我们的模型在做什么？如何训练？

我们的目标是让一个搭载机械臂和灵巧手的移动机器人，能够**自主寻找并抓取**目标物体（如树上的果实）。为了降低难度，我们采用了**两阶段训练法 (Two-Stage Training)**。

### 2.1 第一阶段：追踪 (Tracking)

*   **目标**: 训练机器人学会“眼疾手快”地把手掌送到物体附近，并保持悬停或接触。
*   **控制对象**: 
    *   **底盘 (Base)**: 移动位置。
    *   **机械臂 (Arm)**: 调整末端位置和姿态。
    *   *注意：此阶段灵巧手保持固定姿势，不参与训练。*
*   **输入 (Observation)**: 
    *   机器人自身状态（底盘速度、手臂角度）。
    *   目标物体的位置。
*   **输出 (Action)**: 底盘速度指令 + 机械臂关节速度指令。
*   **奖励机制**: 
    *   离物体越近，分数越高。
    *   手掌触碰到物体，获得高分奖励。
*   **训练结果**: 得到一个能熟练控制底盘和手臂接近目标的“追踪大脑”。

### 2.2 第二阶段：抓取 (Catching) —— 关键阶段

*   **目标**: 在第一阶段的基础上，学会用灵巧手把物体抓住并提起来。
*   **控制对象**:
    *   **底盘 & 机械臂**: **由第一阶段的“追踪大脑”接管**。
    *   **灵巧手 (Hand)**: **由新的“抓取大脑”控制**。
*   **训练流程**:
    1.  **加载**: 系统读取第一阶段训练好的模型权重。
    2.  **冻结**: 锁死“追踪大脑”的参数，不让它在这一阶段发生变化。它就像一个训练有素的司机，只负责把车开到目的地。
    3.  **学习**: 新的“抓取大脑”观察当前状态（包括手臂的位置、物体的位置），学习如何配合底盘和手臂的运动，适时地张开和闭合手指，完成抓取。
*   **奖励机制**:
    *   **手指接近**: 指尖距离物体表面越近越好。
    *   **接触**: 手指接触物体有奖励。
    *   **力闭合/抬起**: 最核心的奖励。如果物体被成功抓握并抬起（高度变化），获得巨额奖励。
*   **Sim2Real (仿真到真机) 优化**:
    *   为了让模型适应真实世界的误差，我们在训练中故意加入了**观测噪声**（模拟传感器误差）和**动作延迟**（模拟通信延迟）。
    *   对于静态物体，我们强制模型忽略物体的速度信息，只依赖位置信息，这样在真机上更稳定。

## 3. 总结

通过这种**解耦**的设计，我们将一个复杂的“移动抓取”任务拆解成了两个相对简单的子任务：
1.  **“走过去，手伸过去”** (Tracking)
2.  **“手抓紧，提起来”** (Catching)

这种方法不仅训练效率更高，而且模块化程度好，便于后续在真实机器人上进行调试和部署。
